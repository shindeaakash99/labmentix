{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Paisa Bazaar Banking Fraud Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":Developed a machine learning model to classify individuals’ credit scores (Good, Standard, Poor) using financial and behavioral data such as income, loan history, delayed payments, and credit utilization. Performed data preprocessing, feature engineering, and exploratory analysis on 100,000 records. Applied classification models including Logistic Regression, Random Forest, and XGBoost, with the best model achieving high accuracy in predicting credit scores. The solution helps financial institutions like Paisabazaar improve credit risk assessment, reduce loan defaults, and provide personalized financial recommendations."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/shindeaakash99"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Financial institutions struggle to accurately evaluate customer creditworthiness, leading to loan defaults and missed opportunities. Paisabazaar needs a machine learning model to classify credit scores (Good, Standard, Poor) using demographic, financial, and behavioral data, enabling better risk management and personalized financial recommendations."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('dataset-2.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df.isnull())\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-The dataset has 100,000 rows and 28 columns with no missing or duplicate values.\n",
        "\n",
        "-It contains both numerical (Age, Annual Income, Outstanding Debt, etc.) and categorical features (Occupation, Credit Mix, Payment Behaviour).\n",
        "\n",
        "-The target variable is Credit_Score (Good, Standard, Poor), making it suitable for classification."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "variables_description = {\n",
        "    'ID': 'Unique identifier for each record',\n",
        "    'Customer_ID': 'Unique ID of the customer',\n",
        "    'Month': 'Month number (1-12) when data is recorded',\n",
        "    'Name': 'Name of the customer',\n",
        "    'Age': 'Age of the customer in years',\n",
        "    'SSN': 'Social Security Number (masked)',\n",
        "    'Occupation': 'Occupation of the customer',\n",
        "    'Annual_Income': 'Customer\\'s annual income',\n",
        "    'Monthly_Inhand_Salary': 'Salary received per month after deductions',\n",
        "    'Num_Bank_Accounts': 'Number of bank accounts held by the customer',\n",
        "    'Num_Credit_Card': 'Number of credit cards held',\n",
        "    'Interest_Rate': 'Interest rate for loans or credit',\n",
        "    'Num_of_Loan': 'Number of active loans',\n",
        "    'Type_of_Loan': 'Type/category of the loan',\n",
        "    'Delay_from_due_date': 'Average delay from due date in days',\n",
        "    'Num_of_Delayed_Payment': 'Number of delayed payments',\n",
        "    'Changed_Credit_Limit': 'Whether credit limit was changed (Yes/No)',\n",
        "    'Num_Credit_Inquiries': 'Number of credit inquiries made',\n",
        "    'Credit_Mix': 'Type of credit mix (Good/Bad/Standard)',\n",
        "    'Outstanding_Debt': 'Current outstanding debt',\n",
        "    'Credit_Utilization_Ratio': 'Ratio of credit used to total credit limit',\n",
        "    'Credit_History_Age': 'Age of credit history in months',\n",
        "    'Payment_of_Min_Amount': 'Whether minimum payment was made (Yes/No)',\n",
        "    'Total_EMI_per_month': 'Total EMI paid per month',\n",
        "    'Amount_invested_monthly': 'Amount invested monthly by the customer',\n",
        "    'Payment_Behaviour': 'Customer\\'s payment behavior category',\n",
        "    'Monthly_Balance': 'Remaining balance at month-end',\n",
        "    'Credit_Score': 'Credit score of the customer (Good/Standard/Bad)'\n",
        "}\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].unique()[:10]} ...\")   # Show first 10 unique values for brevity"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or use this code for full uniques names\n",
        "for col in df.columns:\n",
        "    print(col, df[col].unique())"
      ],
      "metadata": {
        "id": "HDkwBfT6s62P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "YhiwAoH7yICg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Wrangling Steps\n",
        "#1. Check duplicates\n",
        "#2. Check missing values\n",
        "#3. Check data typess\n",
        "#4. Standardize column names(if needed)\n",
        "#5. Encode binary categorical columns\n",
        "#6. Encode multi-class categorical variables using one-hot encoding\n",
        "#7. Data after Wrangling check---df.info(), df.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant / identifier columns\n",
        "irrelevant_cols = ['ID', 'Customer_ID', 'Name', 'SSN']\n",
        "df = df.drop(columns=irrelevant_cols)\n",
        "print(\"Dropped columns:\", irrelevant_cols)"
      ],
      "metadata": {
        "id": "czg_hwzCzJEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data types\n",
        "df['Credit_History_Age'] = df['Credit_History_Age'].astype(int)"
      ],
      "metadata": {
        "id": "A-O7YJvMzSc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode binary categorical columns\n",
        "binary_columns = ['Changed_Credit_Limit', 'Payment_of_Min_Amount']\n",
        "for col in binary_columns:\n",
        "    df[col] = df[col].map({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "srrhcqhnzSyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode multi-class categorical variables using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Occupation', 'Type_of_Loan', 'Credit_Mix', 'Payment_Behaviour'], drop_first=True)"
      ],
      "metadata": {
        "id": "EcVXUM5h0Yv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "KyuBmFkm0rx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Dataset Info\n",
        "print(\"\\n### Data After Wrangling ###\")\n",
        "print(df.info())\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "hXNj6G5D07yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removed duplicates and handled missing values\n",
        "\n",
        "#Converted binary columns (Changed_Credit_Limit, Payment_of_Min_Amount) → numeric (Yes=1, No=0)\n",
        "\n",
        "#Applied one-hot encoding to multi-class columns (Occupation, Type_of_Loan, Credit_Mix, Payment_Behaviour, Credit_Score)\n",
        "\n",
        "#Optimized data types → final dataset: 100,000 rows × 6,306 columns, memory ~616 MB"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# 1. Age Distribution - Histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Age'], bins=30, kde=True, color='skyblue')\n",
        "plt.title(\"Age Distribution of Customers\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is best for showing the distribution of a continuous variable like Age, helping to spot trends and outliers easily"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers are aged 25–45, with a slightly right-skewed distribution and no major outliers."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Targeting mid-age customers can improve credit product offerings and risk models"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Annual Income Distribution - KDE\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.kdeplot(df['Annual_Income'], fill=True, color='green')\n",
        "plt.title(\"Annual Income Distribution\")\n",
        "plt.xlabel(\"Annual Income\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A KDE plot is ideal for visualizing the distribution of continuous variables like Annual Income. It helps identify the shape of the data, skewness, and presence of outliers more clearly than a histogram."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers have moderate annual incomes, with the distribution slightly right-skewed, indicating a smaller group with very high incomes."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes. Understanding income distribution helps in segmenting customers, setting suitable credit limits, and designing financial products.\n",
        "The skew towards lower incomes could highlight a potential risk group that may require stricter credit checks, while the high-income group indicates potential for premium offerings."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Monthly Inhand Salary Distribution - Histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Monthly_Inhand_Salary'], bins=30, color='orange')\n",
        "plt.title(\"Monthly Inhand Salary Distribution\")\n",
        "plt.xlabel(\"Monthly Salary\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is best for understanding the frequency distribution of a continuous variable like monthly salary. It clearly shows how values are spread across different ranges"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of customers fall within a moderate monthly salary range, with fewer individuals earning very high amounts"
      ],
      "metadata": {
        "id": "dbAExE7P4pGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Knowing the salary distribution helps in credit risk assessment, loan eligibility design, and customer segmentation.\n",
        "A concentration of lower salaries may indicate potential risk segments needing stricter lending policies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#Number of Loans - Boxplot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(y=df['Num_of_Loan'], color='purple')\n",
        "plt.title(\"Distribution of Number of Loans\")\n",
        "plt.ylabel(\"Number of Loans\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxplot is ideal for visualizing the spread, central tendency, and outliers of numerical data like the number of loans. It clearly shows the median, quartiles, and extreme values"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers have a moderate number of loans, but the boxplot shows the presence of outliers, indicating some customers have significantly more loans than the average"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Identifying customers with unusually high numbers of loans can help in assessing credit risk and preventing over-leveraging.\n",
        "Outliers may indicate high-risk individuals needing stricter credit checks."
      ],
      "metadata": {
        "id": "a9QJcJcY6iCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Interest Rate vs Number of Loans - Scatter Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='Num_of_Loan', y='Interest_Rate', data=df, alpha=0.5)\n",
        "plt.title(\"Interest Rate vs Number of Loans\")\n",
        "plt.xlabel(\"Number of Loans\")\n",
        "plt.ylabel(\"Interest Rate\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is ideal to visualize the relationship between two continuous variables — here, number of loans and interest rate. It helps in identifying patterns, correlations, or clusters in the data"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a slight upward trend where customers with more loans tend to have higher interest rates, indicating increased credit risk. The points are scattered, suggesting variability but some relationship"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding this relationship helps in interest rate determination and risk assessment. Customers with multiple loans may be charged higher interest rates to compensate for increased risk.\n",
        "Very high-interest rates for high-loan individuals may also affect customer retention."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Total EMI per Month vs Annual Income - Scatter Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='Annual_Income', y='Total_EMI_per_month', data=df, alpha=0.5, color='red')\n",
        "plt.title(\"Annual Income vs Total EMI per Month\")\n",
        "plt.xlabel(\"Annual Income\")\n",
        "plt.ylabel(\"Total EMI per Month\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is ideal for examining the relationship between two numerical variables, in this case, annual income and total EMI per month. It helps visualize how EMI obligations vary with income levels."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows that individuals with higher annual incomes tend to have higher EMIs, which is expected since they may take larger loans. However, there’s noticeable spread, indicating variability in EMI even among similar income groups"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding this relationship helps in loan eligibility assessment and setting appropriate EMI-to-income ratios.\n",
        "Customers with high EMIs relative to their income may represent higher credit risk."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Number of Credit Cards - Boxplot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(y='Num_Credit_Card', data=df, color='teal')\n",
        "plt.title(\"Distribution of Number of Credit Cards\")\n",
        "plt.ylabel(\"Number of Credit Cards\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxplot is useful for visualizing the spread, median, and outliers of numerical data like the number of credit cards. It clearly shows variability across customers"
      ],
      "metadata": {
        "id": "3HGy7PM78N1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers hold a moderate number of credit cards, but the plot reveals the presence of outliers, indicating that some customers have significantly more cards than average"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This information helps in identifying potential high-risk individuals with multiple credit lines and can assist in credit limit management.\n",
        "Customers with an unusually high number of credit cards might represent higher financial exposure, requiring closer monitoring."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Number of Loans vs Age - Strip Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.stripplot(x='Num_of_Loan', y='Age', data=df, jitter=0.3, palette='Set1')\n",
        "plt.title('Number of Loans vs Age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A strip plot is ideal for showing the distribution of individual data points across categories. It helps visualize how age varies with different numbers of loans"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows that individuals across various age groups have taken different numbers of loans. Middle-aged individuals appear to have a higher concentration of multiple loans, while younger and older groups are more scattered"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This insight can help in age-based loan targeting and risk profiling.\n",
        "⚠️ Younger customers with many loans may represent potential credit risk, requiring tailored financial products or stricter eligibility checks"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Delay from Due Date - Histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Delay_from_due_date'], bins=25, color='brown')\n",
        "plt.title(\"Distribution of Payment Delay (Days)\")\n",
        "plt.xlabel(\"Delay from Due Date\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is best for understanding the frequency distribution of numerical data. Here, it helps analyze how often customers delay payments and by how many days"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers tend to have short or no payment delays, but there’s a tail towards higher delays, indicating some customers frequently delay payments for longer periods"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Identifying customers with longer delays can help financial institutions assess credit risk and adjust interest rates or credit limits accordingly.\n",
        "A cluster of customers with frequent long delays may signal potential default risks"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Outstanding Debt vs Annual Income - Scatter Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='Annual_Income', y='Outstanding_Debt', data=df, alpha=0.5, color='magenta')\n",
        "plt.title(\"Outstanding Debt vs Annual Income\")\n",
        "plt.xlabel(\"Annual Income\")\n",
        "plt.ylabel(\"Outstanding Debt\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is ideal for visualizing the relationship between two continuous variables. Here, it helps examine how outstanding debt levels vary with annual income"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows a positive trend — customers with higher annual incomes generally tend to have higher outstanding debts, which suggests they may have taken larger loans or credit facilities. However, there is some variability across income groups"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding this relationship helps in risk assessment and setting appropriate debt-to-income ratios for different customer segments.\n",
        "High outstanding debts among low-income groups can indicate potential repayment risks, requiring closer monitoring."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Credit Utilization Ratio - Histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Credit_Utilization_Ratio'], bins=30, color='cyan')\n",
        "plt.title(\"Credit Utilization Ratio Distribution\")\n",
        "plt.xlabel(\"Credit Utilization Ratio\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is perfect for understanding the distribution of numerical data like credit utilization ratio. It clearly shows how customers are using their available creditAnswer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution shows that most customers have moderate credit utilization, while some have very high utilization ratios, indicating they are using a large portion of their credit limit"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. High credit utilization is often associated with higher credit risk, so these insights help in credit score modeling, risk profiling, and credit limit management.\n",
        "Customers with consistently high utilization may need closer monitoring to reduce default risk."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Annual Income vs Monthly Balance - Line Plot (sample)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(x='Annual_Income', y='Monthly_Balance', data=df.sample(500))\n",
        "plt.title(\"Annual Income vs Monthly Balance Trend\")\n",
        "plt.xlabel(\"Annual Income\")\n",
        "plt.ylabel(\"Monthly Balance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot is useful for observing trends and patterns between two continuous variables. Here, it helps visualize how monthly balance changes as annual income increases (on a sample to keep it clear)."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trend suggests that monthly balance generally increases with higher annual income, though there are fluctuations indicating variability among individuals with similar incomes"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This relationship helps in understanding savings behavior and can be used for financial planning, creditworthiness assessment, or offering tailored products.\n",
        "Individuals with high income but low monthly balance may signal higher spending or debt burden, requiring careful analysis."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Credit Mix distribution (donut chart)\n",
        "credit_counts = df[['Credit_Mix_Good', 'Credit_Mix_Standard']].sum()\n",
        "credit_counts['Credit_Mix_Poor'] = len(df) - credit_counts.sum()\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(credit_counts, labels=credit_counts.index, autopct='%1.1f%%', startangle=90, colors=['green','orange','skyblue'], wedgeprops={'width':0.4})\n",
        "plt.title(\"Credit Mix Distribution (Donut Chart)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A donut (pie) chart is ideal for showing proportions of categories. Here, it visually represents the distribution of different credit mix categories (Good, Standard, Poor) in the dataset."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer HereThe majority of customers fall under the Good and Standard credit mix categories, while a smaller portion has a Poor credit mix"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Knowing the credit mix distribution helps in credit scoring, risk segmentation, and targeted financial products.\n",
        "A higher proportion of poor credit mix could indicate potential default risk segments that need more monitoring."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#Compute correlation for numeric columns only\n",
        "corr = df.select_dtypes(include=['float64', 'int64', 'int32']).corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, cmap='coolwarm', cbar=True)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is ideal for identifying relationships between multiple numerical variables at once. It provides a quick overview of how strongly features are related to each other\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows that certain financial variables like Annual Income, Outstanding Debt, Number of Loans, and Credit Utilization Ratio exhibit moderate correlations with each other. Some features have weak correlations, indicating independence"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Pairplot (key numeric features)\n",
        "pair_cols = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_of_Loan', 'Outstanding_Debt', 'Credit_Utilization_Ratio']\n",
        "sns.pairplot(df[pair_cols])\n",
        "plt.suptitle(\"Pairplot of Key Financial Features\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot is excellent for visualizing pairwise relationships between multiple numeric variables simultaneously. It shows both distributions (diagonal) and scatter plots (off-diagonal), making it easy to spot trends and correlations"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot reveals patterns such as positive relationships between income and debt, clusters in loan count vs age, and the overall spread of numeric variables. It also highlights which variables are linearly related and which are not."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers with a Good Credit Mix have a higher Monthly Balance than customers with a Standard Credit Mix"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1: Annual Income vs Total EMI\n",
        "\n",
        "Statement: Customers with higher annual income tend to have higher total EMI per month.\n",
        "\n",
        "Null Hypothesis (H₀): There is no significant correlation between Annual Income and Total EMI per month.\n",
        "\n",
        "Alternative Hypothesis (H₁): There is a significant correlation between Annual Income and Total EMI per month."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Hypothesis 1: Correlation between Annual Income and Total EMI per month\n",
        "corr, p_value = pearsonr(df['Annual_Income'], df['Total_EMI_per_month'])\n",
        "print(f\"Pearson Correlation: {corr:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H₀ → There is a significant correlation.\")\n",
        "else:\n",
        "    print(\"Fail to Reject H₀ → No significant correlation.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to find the correlation between two continuous variables (e.g., Annual Income and Total EMI per Month)"
      ],
      "metadata": {
        "id": "IfNHfQRzCu06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 2: Delay in Payment vs Credit Score Category\n",
        "\n",
        "Statement: The average payment delay is different among customers with different credit score categories.\n",
        "\n",
        "Null Hypothesis (H₀): The mean payment delay is the same across all credit score categories.\n",
        "\n",
        "Alternative Hypothesis (H₁): At least one category has a different mean payment delay."
      ],
      "metadata": {
        "id": "IxkxtjLFCo5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Group the data by Credit_Score categories and extract Delay_from_due_date\n",
        "groups = [group['Delay_from_due_date'].values for name, group in df.groupby('Credit_Score')]\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = f_oneway(*groups)\n",
        "print(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H₀ → There is a significant difference in payment delay across credit score groups.\")\n",
        "else:\n",
        "    print(\"Fail to Reject H₀ → No significant difference between the groups.\")"
      ],
      "metadata": {
        "id": "Fh-B-ff1CiSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to check if the mean payment delay differs significantly across Credit Score groups"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 3: Number of Loans differs by Age Group\n",
        "\n",
        "Statement: Younger customers tend to have fewer loans compared to middle-aged and older customers.\n",
        "\n",
        "Null Hypothesis (H₀): There is no difference in the number of loans between different age groups.\n",
        "\n",
        "Alternative Hypothesis (H₁): There is a difference in the number of loans between different age groups.\n",
        "\n",
        "Test to use: ANOVA or Kruskal-Wallis (if non-normal)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Create age groups\n",
        "df['Age_Group'] = pd.cut(df['Age'], bins=[18, 30, 50, 80], labels=['Young', 'Middle', 'Older'])\n",
        "\n",
        "# Group by age groups\n",
        "young = df[df['Age_Group']=='Young']['Num_of_Loan']\n",
        "middle = df[df['Age_Group']=='Middle']['Num_of_Loan']\n",
        "older = df[df['Age_Group']=='Older']['Num_of_Loan']\n",
        "\n",
        "# Hypothesis 3: ANOVA for Number of Loans by Age Group\n",
        "f_stat, p_value = f_oneway(young, middle, older)\n",
        "print(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H₀ → There is a significant difference in number of loans among age groups.\")\n",
        "else:\n",
        "    print(\"Fail to Reject H₀ → No significant difference among age groups.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to test if the number of loans differs across Age Groups"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target variable\n",
        "TARGET = 'Credit_Score'\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which numeric columns have all NaN values\n",
        "all_nan_cols = [col for col in X.select_dtypes(include=['number']).columns if X[col].isna().all()]\n",
        "print(\"Columns with all NaN values:\", all_nan_cols)\n",
        "\n",
        "# Drop columns with all NaN values (e.g., Changed_Credit_Limit)\n",
        "if len(all_nan_cols) > 0:\n",
        "    X = X.drop(columns=all_nan_cols)"
      ],
      "metadata": {
        "id": "WlQ-0VRs_ATZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate Numeric and Categorical Columns\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(\"Numeric columns:\", len(num_cols))\n",
        "print(\"Categorical columns:\", len(cat_cols))"
      ],
      "metadata": {
        "id": "Av2Y_C8h_Swy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Impute Missing Values (Median for Numeric, Mode for Categorical)\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Numeric columns → Median Imputation\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_num = pd.DataFrame(num_imputer.fit_transform(X[num_cols]), columns=num_cols)\n",
        "\n",
        "# Categorical columns → Mode Imputation\n",
        "if len(cat_cols) > 0:\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X_cat = pd.DataFrame(cat_imputer.fit_transform(X[cat_cols]), columns=cat_cols)\n",
        "    X = pd.concat([X_num, X_cat], axis=1)\n",
        "else:\n",
        "    X = X_num.copy()\n",
        "\n",
        "# Final check\n",
        "print(\"Total Missing Values after Imputation:\", X.isna().sum().sum())\n",
        "print(\"X shape:\", X.shape)"
      ],
      "metadata": {
        "id": "9FdMcuEh_sht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used median imputation for numeric columns to handle skewed data and outliers, and mode imputation for categorical columns to fill missing categories. The Changed_Credit_Limit column was dropped as it contained all missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to avoid modifying original data unintentionally\n",
        "X_out = X.copy()\n",
        "\n",
        "# Loop through each numeric column\n",
        "for col in X_out.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    Q1 = X_out[col].quantile(0.25)\n",
        "    Q3 = X_out[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define lower and upper bounds\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Cap the outliers instead of removing them\n",
        "    X_out[col] = X_out[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "print(\"Outlier treatment completed using IQR capping\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the IQR (Interquartile Range) method to detect and treat outliers in numeric columns.\n",
        "\n",
        "This technique was chosen because it is simple, robust, and effectively identifies extreme values without being influenced by the overall data distribution.\n",
        "\n",
        "Outliers were capped at the upper and lower IQR boundaries to reduce their impact on model performance while retaining valuable data."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy X to avoid altering original\n",
        "X_encoded = X.copy()\n",
        "\n",
        "#1 Identify categorical columns\n",
        "cat_cols = X_encoded.select_dtypes(include=['object']).columns\n",
        "\n",
        "#2 Encode binary categorical columns using LabelEncoder\n",
        "label_enc = LabelEncoder()\n",
        "\n",
        "for col in cat_cols:\n",
        "    unique_vals = X_encoded[col].nunique()\n",
        "    if unique_vals == 2:\n",
        "        X_encoded[col] = label_enc.fit_transform(X_encoded[col])\n",
        "        print(f\"Label Encoded (binary): {col}\")\n",
        "\n",
        "#3 Encode multi-class categorical columns using One-Hot Encoding\n",
        "multi_class_cols = [col for col in cat_cols if X_encoded[col].nunique() > 2]\n",
        "\n",
        "if len(multi_class_cols) > 0:\n",
        "    X_encoded = pd.get_dummies(X_encoded, columns=multi_class_cols, drop_first=True)\n",
        "    print(f\" One-Hot Encoded (multi-class): {multi_class_cols}\")\n",
        "\n",
        "# Final check\n",
        "print(\" Encoding completed.\")\n",
        "print(\"Encoded Data Shape:\", X_encoded.shape)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_encoded.copy()\n",
        "X"
      ],
      "metadata": {
        "id": "BwaDpdXnH523"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Label Encoding for binary categorical columns and One-Hot Encoding for multi-class categorical columns.\n",
        "\n",
        "Label Encoding was applied to features with only two categories (e.g., Yes/No) to convert them into 0 and 1 efficiently.\n",
        "\n",
        "One-Hot Encoding was used for multi-class categorical features to avoid introducing any ordinal relationship between categories, ensuring the model can interpret them correctly."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Dropping highly correlated or irrelevant columns\n",
        "# - Creating new meaningful features if needed\n",
        "\n",
        "# Check correlation to identify highly correlated features\n",
        "corr_matrix = X.corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Drop features with correlation > 0.9\n",
        "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
        "X = X.drop(columns=high_corr_features)\n",
        "\n",
        "print(\"Dropped highly correlated features:\", high_corr_features)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We removed features with very high correlation to reduce multicollinearity and avoid overfitting.\n"
      ],
      "metadata": {
        "id": "dgNZAZPBJ1hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used correlation analysis to remove highly correlated features and domain knowledge to keep meaningful variables. This helps reduce redundancy, improve model interpretability, and prevent overfitting."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features such as Annual_Income, Outstanding_Debt, Num_of_Loan, Credit_Utilization_Ratio, and Monthly_Inhand_Salary were found important because they directly affect credit score and financial risk."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I used Label Encoding for binary categorical columns and One-Hot Encoding for multi-class categorical columns.\n",
        "\n",
        "This transformation is necessary to convert categorical data into numeric format so that machine learning models can process it effectively."
      ],
      "metadata": {
        "id": "kNZZ07EoLn7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Data scaling completed using StandardScaler\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler to standardize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "This helps models like Logistic Regression and SVM converge faster and treat all features equally."
      ],
      "metadata": {
        "id": "i_AEhnwpL9xY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, dimensionality reduction was not needed because the number of features was manageable and feature selection had already reduced redundant variables."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train-Test Split Completed\")\n",
        "print(\"Train Shape:\", X_train.shape)\n",
        "print(\"Test Shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split. 80% of data is used for training to capture patterns, and 20% for testing to evaluate model performance on unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is not highly imbalanced.\n",
        "\n",
        "The distribution of credit score categories is fairly even, so balancing techniques were not applied."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data is not significantly imbalanced, no resampling technique (like SMOTE or class weights) was needed."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "#  Initialize Model\n",
        "model_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Fit Model\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "#  Predict\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters from GridSearch:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate tuned model\n",
        "best_lr = grid_search.best_estimator_\n",
        "y_pred_best = best_lr.predict(X_test)\n",
        "\n",
        "# Accuracy & Report\n",
        "tuned_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Tuned Logistic Regression Accuracy:\", tuned_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
        "\n",
        "# Confusion Matrix for Tuned Model\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_best), annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - Tuned Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter tuning, as it systematically searches across parameter combinations using cross-validation. The best parameters found were C = 10 and solver = lbfgs.\n",
        "\n",
        "After tuning, the model achieved an accuracy of 62.36%, with improved performance in the “Standard” class but limited gains for “Good” and “Poor” categories.\n",
        "\n",
        "This indicates tuning helped optimize parameters but the model may need more feature engineering or a non-linear model for better performance."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "print(\" Baseline Random Forest Accuracy:\", rf_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Baseline Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=25,              # Number of parameter settings sampled\n",
        "    cv=3,                   # Cross-validation folds\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy')\n",
        "\n",
        "# Fit the random search\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters for Random Forest:\", rf_random_search.best_params_)\n",
        "\n",
        "# Evaluate tuned model\n",
        "best_rf = rf_random_search.best_estimator_\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n",
        "\n",
        "tuned_rf_accuracy = accuracy_score(y_test, y_pred_best_rf)\n",
        "print(\" Tuned Random Forest Accuracy:\", tuned_rf_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best_rf))\n",
        "\n",
        "# Confusion Matrix for Tuned Model\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_best_rf), annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - Tuned Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i used RandomizedSearchCV for hyperparameter optimization because it efficiently explores a wide range of parameter combinations and is faster than GridSearchCV, making it suitable for large datasets."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes . After tuning, the Random Forest model achieved 83% accuracy, outperforming Logistic Regression (62%)\n",
        "\n",
        "This indicates that Random Forest captured the underlying patterns in the data more effectively for this problem."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode Target Variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode target labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Check mapping\n",
        "print(\" Class Mapping:\")\n",
        "for i, cls in enumerate(label_encoder.classes_):\n",
        "    print(f\"{cls} → {i}\")"
      ],
      "metadata": {
        "id": "Xqk5NKmtvv8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-split the Data with Encoded y\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize XGBoost model\n",
        "xgb_model = XGBClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\" Baseline XGBoost Accuracy:\", xgb_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - Baseline XGBoost\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IZvk-LnmwUs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define parameter search space\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.3],\n",
        "    'min_child_weight': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV with XGBoost\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        use_label_encoder=False\n",
        "    ),\n",
        "    param_distributions=xgb_param_dist,\n",
        "    n_iter=15,         # Number of random combinations\n",
        "    scoring='accuracy',\n",
        "    cv=3,             # 3-fold Cross Validation\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the randomized search on training data\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters for XGBoost:\", xgb_random_search.best_params_)\n",
        "\n",
        "# Evaluate tuned model\n",
        "best_xgb = xgb_random_search.best_estimator_\n",
        "y_pred_best_xgb = best_xgb.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy & report\n",
        "tuned_xgb_accuracy = accuracy_score(y_test, y_pred_best_xgb)\n",
        "print(\" Tuned XGBoost Accuracy:\", tuned_xgb_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best_xgb, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_best_xgb),\n",
        "            annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix - Tuned XGBoost\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "It explores a wide range of parameter combinations efficiently and is faster than GridSearchCV, making it well-suited for large datasets like this one."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes . After tuning, the XGBoost model’s accuracy improved to 81%, with balanced precision and recall across all classes.\n",
        "\n",
        "This is a significant improvement from Logistic Regression (62%) and Random Forest (78-82%)."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered Accuracy, Precision, Recall, and F1-score. These metrics provide a balanced understanding of the model’s performance.\n",
        "\n",
        "High recall ensures fewer good customers are misclassified, and high precision reduces financial risk by minimizing false approvals."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Tuned Random Forest model as the final model\n",
        "\n",
        "It achieved the highest accuracy (83%) and provided balanced performance across all credit score categories.\n",
        "\n",
        "It also handles non-linear relationships and is less prone to overfitting compared to boosting in this scenario."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Random Forest feature importances to explain the model.\n",
        "\n",
        "Key features such as Annual_Income, Credit_Utilization_Ratio, and Outstanding_Debt had the highest importance, aligning well with real-world credit scoring logic."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # y has strings initially\n",
        "\n",
        "# Save both model and label encoder\n",
        "joblib.dump(best_rf, \"best_random_forest_model.joblib\")\n",
        "joblib.dump(label_encoder, \"label_encoder.joblib\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we built and compared multiple machine learning models to predict credit score categories using customer financial data.\n",
        "\n",
        "After preprocessing and feature engineering, Logistic Regression, Random Forest, and XGBoost were implemented.\n",
        "\n",
        "The Tuned Random Forest model achieved the highest accuracy of 83%, outperforming other models with balanced precision and recall across all classes.\n",
        "\n",
        "The model and label encoder were saved for deployment, and predictions on unseen data confirmed its reliability.\n",
        "\n",
        "This solution can support financial institutions in making data-driven credit decisions and minimizing lending risks."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}